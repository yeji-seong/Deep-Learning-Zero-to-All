{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LAB11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x202bdaf7550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "image = np.array([[[[1],[2],[3]],\n",
    "                   [[4],[5],[6]], \n",
    "                   [[7],[8],[9]]]], dtype=np.float32)\n",
    "print(image.shape)\n",
    "plt.imshow(image.reshape(3,3), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img.shape (1, 2, 2, 1)\n",
      "[[12. 16.]\n",
      " [24. 28.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(2,2))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(2,2), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 1)\n",
      "conv2d_img.shape (1, 3, 3, 1)\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMUAAAC7CAYAAADVEFpBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAJXklEQVR4nO3df6jddR3H8ecrp7eh1VazNuaPGQ3JfkB6vSqCjGSgQ5yQwfwjf6BcEKUfFKQFBkFi/VEkC2Ol2I1Qw+I2ZTEULY1Sdh2bOsf0JoHDgXnNraFNbr3743zL43tnd/fu+/l+z9nu6wGH+/3xuff9OYzXvud7vue8v4oIzOxd7+v3BMwGjUNhljgUZolDYZY4FGaJQ2GW1AqFpA9LekTSS9XPxYcY929J26rHxjo1zZqmOtcpJP0AeCMi7pB0C7A4Ir7ZY9z+iDipxjzNWlM3FLuAVRGxR9Iy4A8RcWaPcQ6FHTXqnlN8LCL2AFQ/P3qIce+XNCHpKUlX1Kxp1qgFhxsg6VFgaY9d355DndMi4lVJHwcek/RcRPy1R61RYLRaPmdoaGgOJQbXiSee2O8pFDM1NdXvKZT0ekScnDe28vIp/c69wMMR8eBM4xYuXBgrVqw44rkNkpGRkX5PoZixsbF+T6GkZyJiOG+s+/JpI3BNtXwN8Ls8QNJiSUPV8hLgQuCFmnXNGlM3FHcAqyW9BKyu1pE0LOnn1ZhPAhOStgOPA3dEhENhA+uw5xQziYgp4OIe2yeAG6rlPwOfqVPHrE2+om2WOBRmiUNhljgUZolDYZY4FGaJQ2GWOBRmiUNhljgUZolDYZY4FGaJQ2GWOBRmiUNhljgUZolDYZYUCYWkSyTtkjRZNUXL+4ckPVDtf1rSihJ1zZpQOxSSjgN+AlwKnAVcJemsNOx64B8R8QngR8D369Y1a0qJI8UIMBkRL0fEO8D9wNo0Zi3wi2r5QeBiSSpQ26y4EqFYDrzStb672tZzTERMA3uBj+Q/JGm06iQ4MT09XWBqZnNXIhS9/sfPHdZmM4aI2BARwxExvGBBrUYjZkesRCh2A6d2rZ8CvHqoMZIWAB8C3ihQ26y4EqHYAqyUdIakE4B1dDoHduvuJHgl8Fj4XsU2oGq/RomIaUk3A5uB44B7ImKHpO8CExGxEbgb+KWkSTpHiHV165o1pcgL94jYBGxK227rWv4X8MUStcya5ivaZolDYZY4FGaJQ2GWOBRmiUNhljgUZolDYZY4FGaJQ2GWOBRmiUNhljgUZolDYZY4FGaJQ2GWtNUM7VpJf5e0rXrcUKKuWRNqf/OuqxnaajoNCrZI2hgRL6ShD0TEzXXrmTWtrWZoZkeNEt/R7tUM7bwe474g6SLgReBrEfFKHiBpFBgFWLp0KWNjYwWm13/nnntuv6dQzL59+/o9hWLGx8d7bm+rGdpDwIqI+CzwKO+20HzvL3U1Q1u0aFGBqZnNXSvN0CJiKiIOVKs/A84pUNesEa00Q5O0rGv1cmBngbpmjWirGdqXJV0OTNNphnZt3bpmTWmrGdqtwK0lapk1zVe0zRKHwixxKMwSh8IscSjMEofCLHEozBKHwixxKMwSh8IscSjMEofCLHEozBKHwixxKMwSh8IsKdUM7R5Jr0l6/hD7JenOqlnas5LOLlHXrAmljhT3ApfMsP9SYGX1GAXuKlTXrLgioYiIJ+h89/pQ1gJj0fEUsCg1MzAbGG2dU/RqmLa8pdpmc9JWKGbTMA1Jo5ImJE28+eabLUzL7GBtheKwDdPAHQJtMLQVio3A1dW7UOcDeyNiT0u1zeakSN8nSfcBq4AlknYD3wGOB4iIn9LpCbUGmATeAq4rUdesCaWaoV11mP0B3FSillnTfEXbLHEozBKHwixxKMwSh8IscSjMEofCLHEozBKHwixxKMwSh8IscSjMEofCLHEozBKHwixxKMwSh8IsaatD4CpJeyVtqx63lahr1oQiX0el0yFwPTA2w5gnI+KyQvXMGtNWh0Czo0apI8VsXCBpO51+T9+IiB15gKRROr1mWbhwIbfffnuL02vO8uXHTjPE8fHxfk+hcW2FYitwekTsl7QGGKfTbPk9ImIDsAFg8eLFB3UQNGtDK+8+RcS+iNhfLW8Cjpe0pI3aZnPVSigkLZWkanmkqjvVRm2zuWqrQ+CVwI2SpoG3gXVVgzSzgdNWh8D1dN6yNRt4vqJtljgUZolDYZY4FGaJQ2GWOBRmiUNhljgUZolDYZY4FGaJQ2GWOBRmiUNhljgUZolDYZY4FGZJ7VBIOlXS45J2Stoh6Ss9xkjSnZImJT0r6ey6dc2aUuKbd9PA1yNiq6QPAM9IeiQiXugacymd7h0rgfOAu6qfZgOn9pEiIvZExNZq+Z/ATiA3OloLjEXHU8AiScvq1jZrQtFzCkkrgM8BT6ddy4FXutZ3c3BwkDQqaULSxIEDB0pOzWzWioVC0knAb4CvRsS+vLvHrxzUzSMiNkTEcEQMDw0NlZqa2ZyU6jp+PJ1A/CoifttjyG7g1K71U+i0zzQbOCXefRJwN7AzIn54iGEbgaurd6HOB/ZGxJ66tc2aUOLdpwuBLwHPSdpWbfsWcBr8vxnaJmANMAm8BVxXoK5ZI2qHIiL+RO9zhu4xAdxUt5ZZG3xF2yxxKMwSh8IscSjMEofCLHEozBKHwixxKMwSh8IscSjMEofCLHEozBKHwixxKMwSh8IscSjMkraaoa2StFfStupxW926Zk1pqxkawJMRcVmBemaNaqsZmtlRo61maAAXSNou6feSPlWyrllJ6vQUKPCHOs3Q/gh8L/d+kvRB4D8RsV/SGuDHEbGyx98YBUar1TOBXUUmN7MlwOst1GnDsfJc2noep0fEyXljkVBUzdAeBjbP0Pupe/zfgOGI6Ps/oKSJiBju9zxKOFaeS7+fRyvN0CQtrcYhaaSqO1W3tlkT2mqGdiVwo6Rp4G1gXZR63WZWWFvN0NYD6+vWasiGfk+goGPlufT1eRQ70TY7VvhjHmbJvA2FpEsk7aruw3dLv+dzpCTdI+k1Sc/3ey51zeYjQ63MYz6+fJJ0HPAisJrOvTO2AFf1+GjKwJN0EbCfzu3TPt3v+dRR3fJtWfdHhoAr2v53ma9HihFgMiJejoh3gPvp3JfvqBMRTwBv9HseJQzKR4bmayhmdQ8+65/DfGSoUfM1FLO6B5/1x2Hun9i4+RoK34NvQM3i/omNm6+h2AKslHSGpBOAdXTuy2d9NMv7JzZuXoYiIqaBm4HNdE7mfh0RO/o7qyMj6T7gL8CZknZLur7fc6rhfx8Z+nzXtzTXtD2JefmWrNlM5uWRwmwmDoVZ4lCYJQ6FWeJQmCUOhVniUJglDoVZ8l/r+wUtQL/ZIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.]],[[1.]]],\n",
    "                      [[[1.]],[[1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,2,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 filters (2,2,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape (1, 3, 3, 1)\n",
      "weight.shape (2, 2, 1, 3)\n",
      "conv2d_img.shape (1, 3, 3, 3)\n",
      "[[12. 16.  9.]\n",
      " [24. 28. 15.]\n",
      " [15. 17.  9.]]\n",
      "[[120. 160.  90.]\n",
      " [240. 280. 150.]\n",
      " [150. 170.  90.]]\n",
      "[[-12. -16.  -9.]\n",
      " [-24. -28. -15.]\n",
      " [-15. -17.  -9.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"imag:\\n\", image)\n",
    "print(\"image.shape\", image.shape)\n",
    "\n",
    "weight = tf.constant([[[[1.,10.,-1.]],[[1.,10.,-1.]]],\n",
    "                      [[[1.,10.,-1.]],[[1.,10.,-1.]]]])\n",
    "print(\"weight.shape\", weight.shape)\n",
    "conv2d = tf.nn.conv2d(image, weight, strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2d_img = conv2d.eval()\n",
    "print(\"conv2d_img.shape\", conv2d_img.shape)\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    print(one_img.reshape(3,3))\n",
    "    plt.subplot(1,3,i+1), plt.imshow(one_img.reshape(3,3), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, 1)\n",
      "[[[[4.]]]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[[[4],[3]],\n",
    "                    [[2],[1]]]], dtype=np.float32)\n",
    "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 1, 1, 1], padding='VALID')\n",
    "print(pool.shape)\n",
    "print(pool.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n",
      "[[[[4.]\n",
      "   [3.]]\n",
      "\n",
      "  [[2.]\n",
      "   [1.]]]]\n"
     ]
    }
   ],
   "source": [
    "image = np.array([[[[4],[3]],\n",
    "                    [[2],[1]]]], dtype=np.float32)\n",
    "pool = tf.nn.max_pool(image, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 1, 1, 1], padding='SAME')\n",
    "print(pool.shape)\n",
    "print(pool.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x202000342e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(1, 14, 14, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "img = img.reshape(-1,28,28,1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 5], stddev=0.01))\n",
    "conv2d = tf.nn.conv2d(img, W1, strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(conv2d)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d_img = conv2d.eval()\n",
    "conv2d_img = np.swapaxes(conv2d_img, 0, 3)\n",
    "for i, one_img in enumerate(conv2d_img):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(14,14), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool:0\", shape=(1, 7, 7, 5), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pool = tf.nn.max_pool(conv2d, ksize=[1, 2, 2, 1], strides=[\n",
    "                        1, 2, 2, 1], padding='SAME')\n",
    "print(pool)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "pool_img = pool.eval()\n",
    "pool_img = np.swapaxes(pool_img, 0, 3)\n",
    "for i, one_img in enumerate(pool_img):\n",
    "    plt.subplot(1,5,i+1), plt.imshow(one_img.reshape(7, 7), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST image conv & pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer1 (conv + ReLU + pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer2 (conv + ReLU + pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\\nTensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2_flat = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 3136), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer3 (fully connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-8d445e21a50d>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.349155440\n",
      "Epoch: 0002 cost = 0.094004515\n",
      "Epoch: 0003 cost = 0.068528525\n",
      "Epoch: 0004 cost = 0.056919450\n",
      "Epoch: 0005 cost = 0.048441664\n",
      "Epoch: 0006 cost = 0.042257123\n",
      "Epoch: 0007 cost = 0.038269878\n",
      "Epoch: 0008 cost = 0.033537909\n",
      "Epoch: 0009 cost = 0.029520348\n",
      "Epoch: 0010 cost = 0.026537905\n",
      "Epoch: 0011 cost = 0.022382688\n",
      "Epoch: 0012 cost = 0.020480449\n",
      "Epoch: 0013 cost = 0.017916016\n",
      "Epoch: 0014 cost = 0.016170256\n",
      "Epoch: 0015 cost = 0.015136602\n",
      "Learning Finished!\n",
      "Accuracy: 0.9891\n",
      "Label:  [4]\n",
      "Prediction:  [4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final FC 7x7x64 inputs -> 10 outputs\n",
    "W3 = tf.get_variable(\"w3\", shape=[7 * 7 * 64, 10],initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L2_flat, W3) + b\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# dropout (keep_prob) rate  0.7~0.5 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "X_img = tf.reshape(X, [-1, 28, 28, 1])   # img 28x28x1 (black/white)\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer1 (conv + ReLU + pool) + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\\nTensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\\nTensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 ImgIn shape=(?, 28, 28, 1)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "#    Conv     -> (?, 28, 28, 32)\n",
    "#    Pool     -> (?, 14, 14, 32)\n",
    "L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer2 (conv + ReLU + pool) + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\\nTensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\\nTensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 ImgIn shape=(?, 14, 14, 32)\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "#    Conv      ->(?, 14, 14, 64)\n",
    "#    Pool      ->(?, 7, 7, 64)\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                    strides=[1, 2, 2, 1], padding='SAME')\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer3 (conv + ReLU + pool) + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\\nTensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\\nTensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L3 ImgIn shape=(?, 7, 7, 64)\n",
    "W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "#    Conv      ->(?, 7, 7, 128)\n",
    "#    Pool      ->(?, 4, 4, 128)\n",
    "#    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
    "L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                    1, 2, 2, 1], padding='SAME')\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "'''\n",
    "Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer4 (fully connected 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\\nTensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L4 FC 4x4x128 inputs -> 625 outputs\n",
    "W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([625]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "'''\n",
    "Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer5 (fully connected 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning started. It takes sometime.\n",
      "Epoch: 0001 cost = 0.428503816\n",
      "Epoch: 0002 cost = 0.089714001\n",
      "Epoch: 0003 cost = 0.066497386\n",
      "Epoch: 0004 cost = 0.055284996\n",
      "Epoch: 0005 cost = 0.048259661\n",
      "Epoch: 0006 cost = 0.043279829\n",
      "Epoch: 0007 cost = 0.038715991\n",
      "Epoch: 0008 cost = 0.035220372\n",
      "Epoch: 0009 cost = 0.033429264\n",
      "Epoch: 0010 cost = 0.030066086\n",
      "Epoch: 0011 cost = 0.029331604\n",
      "Epoch: 0012 cost = 0.029358298\n",
      "Epoch: 0013 cost = 0.024833718\n",
      "Epoch: 0014 cost = 0.026634576\n",
      "Epoch: 0015 cost = 0.023585473\n",
      "Learning Finished!\n",
      "Accuracy: 0.994\n",
      "Label:  [8]\n",
      "Prediction:  [8]\n"
     ]
    }
   ],
   "source": [
    "# L5 Final FC 625 inputs -> 10 outputs\n",
    "W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L4, W5) + b5\n",
    "'''\n",
    "Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "'''\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "print('Learning started. It takes sometime.')\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with Deep CNN (using class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate는 학습시 0.5~0.7, test 시 1\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # L1 ImgIn shape=(?, 28, 28, 1)\n",
    "            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1], padding='SAME')\n",
    "            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "            Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
    "            Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "            Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L2 ImgIn shape=(?, 14, 14, 32)\n",
    "            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L2 = tf.nn.relu(L2)\n",
    "            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
    "                                strides=[1, 2, 2, 1], padding='SAME')\n",
    "            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "            Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n",
    "            Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "            Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L3 ImgIn shape=(?, 7, 7, 64)\n",
    "            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
    "            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            L3 = tf.nn.relu(L3)\n",
    "            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
    "                                1, 2, 2, 1], padding='SAME')\n",
    "            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n",
    "\n",
    "            L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "            '''\n",
    "            Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "            Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n",
    "            Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "            Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n",
    "            Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L4 FC 4x4x128 inputs -> 625 outputs\n",
    "            W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4 = tf.Variable(tf.random_normal([625]))\n",
    "            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
    "            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n",
    "            Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n",
    "            '''\n",
    "\n",
    "            # L5 Final FC 625 inputs -> 10 outputs\n",
    "            W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b5 = tf.Variable(tf.random_normal([10]))\n",
    "            self.logits = tf.matmul(L4, W5) + b5\n",
    "            '''\n",
    "            Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n",
    "            '''\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, keep_prop=1.0):\n",
    "        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.keep_prob: keep_prop})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n",
    "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n",
    "\n",
    "    def train(self, x_data, y_data, keep_prop=0.7):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Started!\n",
      "Epoch: 0001 cost = 0.399221238\n",
      "Epoch: 0002 cost = 0.095649997\n",
      "Epoch: 0003 cost = 0.071951915\n",
      "Epoch: 0004 cost = 0.061719922\n",
      "Epoch: 0005 cost = 0.052445198\n",
      "Epoch: 0006 cost = 0.048726227\n",
      "Epoch: 0007 cost = 0.043669724\n",
      "Epoch: 0008 cost = 0.039413202\n",
      "Epoch: 0009 cost = 0.035597422\n",
      "Epoch: 0010 cost = 0.037153179\n",
      "Epoch: 0011 cost = 0.031366582\n",
      "Epoch: 0012 cost = 0.029903403\n",
      "Epoch: 0013 cost = 0.028358818\n",
      "Epoch: 0014 cost = 0.027076354\n",
      "Epoch: 0015 cost = 0.027232251\n",
      "Learning Finished!\n",
      "Accuracy: 0.9948\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "M1 = Model(sess, \"M1\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = M1.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "print('Accuracy:', M1.get_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with Deep CNN (using tf.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     padding=\"same\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"same\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=0.5, training=self.training)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-31cc24d7ea1d>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\SAMSUNG\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001984B130080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001984B130080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001984B130080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001984B130080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-5-31cc24d7ea1d>:26: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-5-31cc24d7ea1d>:28: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-5-31cc24d7ea1d>:49: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1DB70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-5-31cc24d7ea1d>:58: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Learning Started!\n",
      "Epoch: 0001 cost = 0.282819925\n",
      "Epoch: 0002 cost = 0.085033742\n",
      "Epoch: 0003 cost = 0.067825228\n",
      "Epoch: 0004 cost = 0.055662428\n",
      "Epoch: 0005 cost = 0.049441969\n",
      "Epoch: 0006 cost = 0.044655167\n",
      "Epoch: 0007 cost = 0.040102849\n",
      "Epoch: 0008 cost = 0.039518334\n",
      "Epoch: 0009 cost = 0.035635593\n",
      "Epoch: 0010 cost = 0.032005913\n",
      "Epoch: 0011 cost = 0.032513373\n",
      "Epoch: 0012 cost = 0.032808140\n",
      "Epoch: 0013 cost = 0.029502209\n",
      "Epoch: 0014 cost = 0.027102123\n",
      "Epoch: 0015 cost = 0.027662345\n",
      "Learning Finished!\n",
      "Accuracy: 0.9936\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "m1 = Model(sess, \"m1\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = m1.train(batch_xs, batch_ys)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "print('Accuracy:', m1.get_accuracy(mnist.test.images, mnist.test.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST with Deep CNN (using ensemble layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    "\n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=0.3, training=self.training)\n",
    "\n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=0.5, training=self.training)\n",
    "\n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
    "\n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    "\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    "\n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    "\n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000019841B1D748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A746E10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A9E6DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A9E6DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A9E6DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dropout.call of <tensorflow.python.layers.core.Dropout object at 0x000001980A9E6DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000001980A9E6DA0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Learning Started!\n",
      "Epoch: 0001 cost = [0.28777907 0.28888161]\n",
      "Epoch: 0002 cost = [0.08703706 0.0891643 ]\n",
      "Epoch: 0003 cost = [0.0671182  0.06541737]\n",
      "Epoch: 0004 cost = [0.0567974  0.05677831]\n",
      "Epoch: 0005 cost = [0.04879164 0.04961579]\n",
      "Epoch: 0006 cost = [0.04373002 0.04499799]\n",
      "Epoch: 0007 cost = [0.04049731 0.04219925]\n",
      "Epoch: 0008 cost = [0.03743892 0.03705682]\n",
      "Epoch: 0009 cost = [0.03574438 0.03623683]\n",
      "Epoch: 0010 cost = [0.03449073 0.03346324]\n",
      "Epoch: 0011 cost = [0.03092399 0.03134695]\n",
      "Epoch: 0012 cost = [0.03126284 0.03004096]\n",
      "Epoch: 0013 cost = [0.02986226 0.029002  ]\n",
      "Epoch: 0014 cost = [0.02829037 0.0267712 ]\n",
      "Epoch: 0015 cost = [0.02716876 0.02402416]\n",
      "Learning Finished!\n",
      "0 Accuracy: 0.9929\n",
      "1 Accuracy: 0.995\n",
      "Ensemble accuracy: 0.9949\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.Session()\n",
    "\n",
    "models = []\n",
    "num_models = 2\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Learning Started!')\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "test_size = len(mnist.test.labels)\n",
    "predictions = np.zeros([test_size, 10])\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
    "        mnist.test.images, mnist.test.labels))\n",
    "    p = m.predict(mnist.test.images)\n",
    "    predictions += p\n",
    "\n",
    "ensemble_correct_prediction = tf.equal(\n",
    "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(\n",
    "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
