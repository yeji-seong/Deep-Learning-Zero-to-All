# Vanishing gradient <br>
초반의 입력값이 sigmoid function에 의해 0에 가까워져 뒤의 값에 거의 영향을 미치지 못하는 문제. <br><br>

# ReLU:Rectified Linear Unit <br>
![figure1](https://user-images.githubusercontent.com/57740560/94715631-b4e0d080-0388-11eb-89c1-210a3a53956b.png) <br>
sigmoid의 Vanishing gradient 해결을 위해 제안됨 <br><br>

# Other Activation functions <br>
![figure2](https://user-images.githubusercontent.com/57740560/94715390-5e739200-0388-11eb-9bfa-9582717049c6.png) <br><br>

# Initialization method <br>
![figure2](https://user-images.githubusercontent.com/57740560/94715903-0ab57880-0389-11eb-9548-73d27612c929.png) <br><br>



[참고 자료 및 이미지 출처] <br>
모두를 위한 딥러닝 시즌1 
