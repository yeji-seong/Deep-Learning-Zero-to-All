# Vanishing gradient <br>
초반의 입력값이 sigmoid function에 의해 0에 가까워져 뒤의 값에 거의 영향을 미치지 못하는 문제. <br>

# ReLU:Rectified Linear Unit <br>
![figure1](https://user-images.githubusercontent.com/57740560/94715386-5d426500-0388-11eb-89bf-8bcf269dff74.png) <br>
sigmoid의 Vanishing gradient 해결을 위해 제안됨 <br><br>

# Other Activation functions <br>
![figure2](https://user-images.githubusercontent.com/57740560/94715390-5e739200-0388-11eb-9bfa-9582717049c6.png) <br><br>

#

[참고 자료 및 이미지 출처] <br>
모두를 위한 딥러닝 시즌1 
